{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM\n",
    "- Multiple countries\n",
    "- Autoregressive training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Real_date\"] = pd.to_datetime(country_data[\"Date_reported\"]) # save the real date for plotting\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = pd.to_datetime(country_data[\"Date_reported\"])\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = (country_data[\"Date_reported\"] - country_data[\"Date_reported\"].iloc[0]).dt.days\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Real_date\"] = pd.to_datetime(country_data[\"Date_reported\"]) # save the real date for plotting\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = pd.to_datetime(country_data[\"Date_reported\"])\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = (country_data[\"Date_reported\"] - country_data[\"Date_reported\"].iloc[0]).dt.days\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Real_date\"] = pd.to_datetime(country_data[\"Date_reported\"]) # save the real date for plotting\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = pd.to_datetime(country_data[\"Date_reported\"])\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1896199140.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  country_data[\"Date_reported\"] = (country_data[\"Date_reported\"] - country_data[\"Date_reported\"].iloc[0]).dt.days\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "path = \"data/COVID_data.csv\"\n",
    "df = pd.read_csv(path)  # Replace with your file path\n",
    "\n",
    "country_waves = {\"Republic of Korea\": {1: [15, 123],\n",
    "                                   2: [206, 295],\n",
    "                                   3: [289, 436],\n",
    "                                   4: [533, 652], #좀 애매함\n",
    "                                   5: [736, 897],\n",
    "                                   6: [897, 1010],\n",
    "                                   7: [1010, 1164] #wave가 애매하게 2개임\n",
    "                                   },\n",
    "            \"China\": {1: [0, 74],\n",
    "                      2: [66, 119],\n",
    "                      3: [154, 266],\n",
    "                      4: [316, 413], #wave가 살짝 2개에 가까움\n",
    "                      5: [484, 561],\n",
    "                      6: [1065, 1119],\n",
    "                      },\n",
    "            \"United Kingdom of Great Britain and Northern Ireland\": {1: [52, 184],\n",
    "                                                                     2: [192, 486], #wave가 2개라서 아래의 3,4번에서 2개로 나눠도 봄.\n",
    "                                                                     3: [192, 330], #2번의 앞쪽 wave\n",
    "                                                                     4: [331, 486], #2번의 뒤쪽 wave\n",
    "                                                                     5: [772, 871],\n",
    "                                                                     6: [869, 983]\n",
    "                   }\n",
    "            }\n",
    "\n",
    "using_coutries = [\"Republic of Korea\", \"China\", \"United Kingdom of Great Britain and Northern Ireland\"]\n",
    "kind = 'New_cases' #[New_cases, Cumulative_cases, New_deaths, Cumulative_deaths]\n",
    "\n",
    "time_data = {\"Republic of Korea\": {}, \"China\": {}, \"United Kingdom of Great Britain and Northern Ireland\": {}}\n",
    "real_time_data = {\"Republic of Korea\": {}, \"China\": {}, \"United Kingdom of Great Britain and Northern Ireland\": {}}\n",
    "wave_data = {\"Republic of Korea\": {}, \"China\": {}, \"United Kingdom of Great Britain and Northern Ireland\": {}}\n",
    "\n",
    "for country in using_coutries:\n",
    "    country_data = df[df[\"Country\"] == country]\n",
    "    country_data[\"Real_date\"] = pd.to_datetime(country_data[\"Date_reported\"]) # save the real date for plotting\n",
    "    country_data[\"Date_reported\"] = pd.to_datetime(country_data[\"Date_reported\"])\n",
    "    country_data[\"Date_reported\"] = (country_data[\"Date_reported\"] - country_data[\"Date_reported\"].iloc[0]).dt.days\n",
    "    new_cases = country_data[kind].values\n",
    "    new_cases[new_cases < 1] = 1\n",
    "\n",
    "    for wave in country_waves[country]:\n",
    "        start, end = country_waves[country][wave]\n",
    "        time_data[country][wave] = country_data[\"Date_reported\"].values[start:end]\n",
    "        real_time_data[country][wave] = country_data[\"Real_date\"].values[start:end]\n",
    "        wave_data[country][wave] = new_cases[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the 7-number moving average\n",
    "def moving_average(data, window_size=7):\n",
    "    half_window = window_size // 2\n",
    "    smoothed = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        # Handle edge cases\n",
    "        start = max(0, i - half_window)\n",
    "        end = min(len(data), i + half_window + 1)\n",
    "        \n",
    "        # Calculate the average of the current window\n",
    "        smoothed.append(np.mean(data[start:end]))\n",
    "    \n",
    "    return smoothed\n",
    "\n",
    "data = {} # smoothed data\n",
    "\n",
    "for country in wave_data:\n",
    "    data[country] = {}\n",
    "    for wave in wave_data[country]:\n",
    "        data[country][wave] = moving_average(wave_data[country][wave])\n",
    "\n",
    "# which countries, which waves to use for training\n",
    "training_coutries = [\"Republic of Korea\", \"China\"]\n",
    "test_coutries = [\"United Kingdom of Great Britain and Northern Ireland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PINN model\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=1):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = num_layers\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :]) # Shape: (batch_size, hidden_dim)\n",
    "        return out\n",
    "\n",
    "# Defining the loss functions\n",
    "def MSE_loss(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def SIR_loss(model, t, beta, gamma, mu):\n",
    "    t = t.requires_grad_(True)  # Enable gradient computation for time tensor\n",
    "\n",
    "    # Forward pass through the model\n",
    "    x = model(t)\n",
    "\n",
    "    # Compute gradients\n",
    "    dx_dt = torch.autograd.grad(\n",
    "        x, t, grad_outputs=torch.ones_like(x), create_graph=True\n",
    "    )[0]  # First derivative\n",
    "\n",
    "    # SIR model loss: dI/dt = beta * I - gamma * I\n",
    "    sir_loss = dx_dt + beta * torch.exp(x) - gamma * torch.exp(x)\n",
    "\n",
    "    # Return the squared loss\n",
    "    return torch.mean(sir_loss ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(data, test_start_point, predicted_values, date_ticks, country, epoch):\n",
    "    \"\"\"\n",
    "    Plot predictions and training results.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    #if not os.path.exists(save_path):\n",
    "    #    os.makedirs(save_path)\n",
    "\n",
    "    predictions = [None]*test_start_point + predicted_values\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(18, 8))\n",
    "    plt.title(f\"{country}. Epoch: {epoch + 1}\")\n",
    "\n",
    "    # Training Data\n",
    "    ax1.plot(data, 'ro', markersize=8, label='Training data')\n",
    "\n",
    "    # Predicted values\n",
    "    ax1.plot(predictions, color=\"orange\", label=\"Predicted by PINN\")\n",
    "\n",
    "    # Format x-axis\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"I(t)\", color=\"k\")\n",
    "    ax1.tick_params(axis='y', labelcolor=\"k\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "\n",
    "    # Set date ticks\n",
    "    tick_labels = date_ticks[:len(data[::7])]\n",
    "    ax1.set_xticks(date_ticks[::7].flatten()) # 없어도 될 것 같음\n",
    "    ax1.set_xticklabels(tick_labels, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience, display_step, sigma, sigma0, mu):\n",
    "        self.patience = patience\n",
    "        self.display_step = display_step\n",
    "        self.model = model\n",
    "        self.sigma = sigma\n",
    "        self.sigma0 = sigma0\n",
    "        self.mu = mu\n",
    "        \n",
    "        self.best_loss = np.inf\n",
    "        self.wait = 0\n",
    "\n",
    "        self.best_model = None\n",
    "        self.bests_weights = None\n",
    "        self.bestSigma = None\n",
    "        self.bestSigma0 = None\n",
    "        self.bestMu = None\n",
    "\n",
    "    def check_early_stopping(self, current_loss, epoch):\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            self.wait = 0\n",
    "\n",
    "            self.best_model = self.model\n",
    "            self.best_weights = self.model.state_dict()\n",
    "            self.bestSigma = self.sigma\n",
    "            self.bestSigma0 = self.sigma0\n",
    "            self.bestMu = self.mu\n",
    "        else:\n",
    "            self.wait += 1\n",
    "        \n",
    "        if self.wait >= self.patience:\n",
    "            print(f\"Early stopping at epoch {epoch}.\")\n",
    "            self.model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1695085361.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_t = torch.tensor(input_t_sequence, dtype=torch.float32).view(-1, 1).to(device)\n",
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1695085361.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(data[i], dtype=torch.float32).view(-1, 1).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE Loss = 0.000991618842817843, SIR Loss = 2.8528040374453667e-08, Average Loss = 0.056915126267584444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lunosoft\\AppData\\Local\\Temp\\ipykernel_13860\\1695085361.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_tensor = torch.tensor(input_sequence, dtype=torch.float32).view(-1, 1).to(device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m t_train \u001b[38;5;241m=\u001b[39m t_data\n\u001b[0;32m    125\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(y_data)  \u001b[38;5;66;03m# Normalize data\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoregressive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreal_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_ticks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwave\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcountry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcountry\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Save the model for this wave\u001b[39;00m\n\u001b[0;32m    144\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wave_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwave\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ar.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 65\u001b[0m, in \u001b[0;36mtrain_autoregressive\u001b[1;34m(model, optimizer, data, t_data, real_t, sigma, sigma0, mu, epochs, patience, display_step, date_ticks, country)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m display_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 65\u001b[0m         predicted_values \u001b[38;5;241m=\u001b[39m \u001b[43mautoregressive_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtest_start_point\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_start_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;66;03m# Ensure dimensions match for plotting\u001b[39;00m\n\u001b[0;32m     67\u001b[0m         predicted_values \u001b[38;5;241m=\u001b[39m predicted_values[:\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m start_point]  \u001b[38;5;66;03m# Slice to match y_data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 104\u001b[0m, in \u001b[0;36mautoregressive_predict\u001b[1;34m(model, given_data, steps, delta_t)\u001b[0m\n\u001b[0;32m    102\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m model(input_tensor)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    103\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mappend(predicted)\n\u001b[1;32m--> 104\u001b[0m         input_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\Lunosoft\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:5611\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5562\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_append_dispatcher)\n\u001b[0;32m   5563\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(arr, values, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   5564\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5565\u001b[0m \u001b[38;5;124;03m    Append values to the end of an array.\u001b[39;00m\n\u001b[0;32m   5566\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5609\u001b[0m \n\u001b[0;32m   5610\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5611\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5613\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Lunosoft\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:1087\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#autoregressive training version\n",
    "def train_autoregressive(model, optimizer, data, t_data, real_t, sigma, sigma0, mu, epochs, patience, display_step, date_ticks, country):\n",
    "    \"\"\"\n",
    "    Train the model using autoregressive learning.\n",
    "    \n",
    "    Parameters:\n",
    "        model: PINN model\n",
    "        optimizer: Optimizer\n",
    "        data: data for training\n",
    "        t_data: time sequence for SIR loss (1,2,3,...)\n",
    "        real_t: real time sequence for plotting (Date Time)\n",
    "        sigma, sigma0, mu: SIR model parameters\n",
    "        epochs: Total epochs\n",
    "        patience: Early stopping patience\n",
    "        display_step: Steps to visualize predictions\n",
    "        date_ticks: Time labels for plots\n",
    "        country: Country name for visualization\n",
    "    \"\"\"\n",
    "    early_stopping = EarlyStopping(model, patience, display_step, sigma, sigma0, mu)\n",
    "    losses = []\n",
    "\n",
    "    # Number of autoregressive steps (how far ahead to predict at each iteration)\n",
    "    start_point = 7  # Predict one week ahead\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        input_sequence = data[:start_point]\n",
    "        input_t_sequence = t_data[:start_point]\n",
    "\n",
    "        input_sequence = list(input_sequence)\n",
    "\n",
    "        for i in range(start_point, len(data)):\n",
    "            input_t = torch.tensor(input_t_sequence, dtype=torch.float32).view(-1, 1).to(device)\n",
    "            input_tensor = torch.tensor(input_sequence, dtype=torch.float32).view(-1, 1).to(device)\n",
    "            target = torch.tensor(data[i], dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                # Forward pass\n",
    "                predicted = model(input_tensor)\n",
    "                mse_loss = MSE_loss(target, predicted)\n",
    "                sir_loss = SIR_loss(model, input_t, sigma, sigma0, mu)\n",
    "                # Combine losses\n",
    "                loss = mse_loss + sir_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad() # Clear gradients\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            input_sequence.append(data[i].cpu().item())\n",
    "\n",
    "        avg_loss = total_loss / (len(data) - start_point)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        test_start_point = int(len(data) / 3)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, MSE Loss = {mse_loss.item()}, SIR Loss = {sir_loss.item()}, Average Loss = {avg_loss}\")\n",
    "\n",
    "        if epoch % display_step == 0:\n",
    "            with torch.no_grad():\n",
    "                predicted_values = autoregressive_predict(model, data[:test_start_point], len(data) - test_start_point, delta_t=1)\n",
    "                # Ensure dimensions match for plotting\n",
    "                predicted_values = predicted_values[:len(data) - start_point]  # Slice to match y_data\n",
    "                plot_predictions(\n",
    "                    data,\n",
    "                    test_start_point,\n",
    "                    predicted_values=predicted_values,\n",
    "                    date_ticks=date_ticks,\n",
    "                    country=country,\n",
    "                    epoch=epoch,\n",
    "                )\n",
    "\n",
    "        if early_stopping.check_early_stopping(avg_loss, epoch):\n",
    "            break\n",
    "\n",
    "    return losses\n",
    "\n",
    "def autoregressive_predict(model, given_data, steps, delta_t):\n",
    "    \"\"\"\n",
    "    Predict future values in an autoregressive manner with multi-step outputs.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained PINN model\n",
    "        data: data for prediction test\n",
    "        steps: Number of future steps to predict\n",
    "        delta_t: Step size for time\n",
    "    \n",
    "    Returns:\n",
    "        Predicted values (list)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_sequence = given_data\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps):\n",
    "            input_tensor = torch.tensor(input_sequence, dtype=torch.float32).view(-1, 1).to(device)\n",
    "            predicted = model(input_tensor).squeeze().cpu().item()\n",
    "            predictions.append(predicted)\n",
    "            input_sequence = np.append(input_sequence, predicted)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Initialize parameters\n",
    "sigma = torch.tensor([0.1], requires_grad=True).to(device)\n",
    "sigma0 = torch.tensor([0.1], requires_grad=True).to(device)\n",
    "mu = torch.tensor([0.1], requires_grad=True).to(device)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = PINN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model using autoregressive learning\n",
    "for country in training_coutries:\n",
    "    for wave in data[country]:\n",
    "        t_data = torch.tensor(time_data[country][wave], dtype=torch.float32).view(-1, 1).to(device)\n",
    "        y_data = torch.tensor(data[country][wave], dtype=torch.float32).view(-1, 1).to(device)\n",
    "        real_t = real_time_data[country][wave] # Real time data for plotting\n",
    "\n",
    "        t_train = t_data\n",
    "        y_train = y_data / max(y_data)  # Normalize data\n",
    "\n",
    "        losses = train_autoregressive(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            data=y_train,\n",
    "            t_data=t_train,\n",
    "            real_t=real_t,\n",
    "            sigma=sigma,\n",
    "            sigma0=sigma0,\n",
    "            mu=mu,\n",
    "            epochs=10000,\n",
    "            patience=1000,\n",
    "            display_step=1000,\n",
    "            date_ticks=time_data[country][wave],\n",
    "            country=country\n",
    "        )\n",
    "\n",
    "        # Save the model for this wave\n",
    "        torch.save(model.state_dict(), f\"models/{country}_wave_{wave}_ar.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
